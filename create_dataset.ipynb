{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b43660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lxml import etree\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1492f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2758d3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = Path(\"data/\")\n",
    "\n",
    "files = [f.name.split('.')[0] for f in dir_path.iterdir() if f.is_file()]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21765bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(file_name):\n",
    "    # Path to TMX file\n",
    "    tmx_path = f\"data/{file_name}.tmx\"\n",
    "\n",
    "    # Parse the TMX XML\n",
    "    tree = etree.parse(tmx_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    rows = []\n",
    "    name = file_name.split('.')[0].split('-')[1]\n",
    "\n",
    "    if name == \"pt\":\n",
    "        lang_name = \"Portuguese\"\n",
    "    elif name == \"fr\":\n",
    "        lang_name = \"French\"\n",
    "    elif name == \"es\":\n",
    "        lang_name = \"Spanish\"\n",
    "    else :\n",
    "        lang_name = \"German\"\n",
    "    \n",
    "    print(f\"Creating the dataframe for {lang_name} language\")\n",
    "    # TMX content is inside <body>\n",
    "    body = root.find(\"body\")\n",
    "\n",
    "    for tu in body.findall(\"tu\"):\n",
    "        en_text = None\n",
    "        second_text = None\n",
    "\n",
    "        for tuv in tu.findall(\"tuv\"):\n",
    "            # Language attribute (xml:lang or lang)\n",
    "            lang = (\n",
    "                tuv.attrib.get(\"{http://www.w3.org/XML/1998/namespace}lang\")\n",
    "                or tuv.attrib.get(\"lang\")\n",
    "            )\n",
    "            seg = tuv.find(\"seg\")\n",
    "            if seg is None:\n",
    "                continue\n",
    "\n",
    "            if lang == \"en\":\n",
    "                en_text = seg.text\n",
    "            elif lang == name:\n",
    "                second_text = seg.text\n",
    "\n",
    "        # Only keep pairs where both languages exist\n",
    "        if en_text and second_text:\n",
    "            rows.append({\n",
    "                \"english\": en_text,\n",
    "                lang_name : second_text\n",
    "            })\n",
    "\n",
    "    # Create DataFrame\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9899d4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "for f in files[::-1]:\n",
    "    print(f\"Starting for langauage {f}\", flush=True)\n",
    "    d = get_data(f)\n",
    "    data[f] = d\n",
    "    ## df3 = df2.merge(df, on='english', how='inner')\n",
    "    print(f\"Done with the language {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cafaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "\n",
    "    # Replace non-breaking space\n",
    "    text = text.replace('\\xa0', ' ')\n",
    "\n",
    "    # Normalize dashes\n",
    "    text = re.sub(r'[‐-–—]', '-', text)\n",
    "\n",
    "    # Normalize quotes\n",
    "    text = text.replace('“', '\"').replace('”', '\"')\n",
    "    text = text.replace('‘', \"'\").replace('’', \"'\")\n",
    "\n",
    "    # Remove replacement character\n",
    "    text = text.replace('�', '')\n",
    "\n",
    "    # Collapse whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50b9f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data:\n",
    "    print(f\"starting for {i}\")\n",
    "    data[i] = data[i].map(clean_text)\n",
    "    data[i] = data[i].sort_values(\"english\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eabb683",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in data.items():\n",
    "    print(f\"The lenght of langauge {key} is {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5994dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df = min(data.values(), key=len)\n",
    "len(base_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02e9edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = base_df.merge(data['en-pt'], how='inner', on='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e19b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df3.drop_duplicates(subset=[\"english\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54990ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.loc[454]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe295b23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fc1492",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df = min(data.values(), key=len)\n",
    "len(base_df)\n",
    "\n",
    "merged_df = base_df.copy()\n",
    "\n",
    "for df in data.values():\n",
    "    if df is not base_df:\n",
    "        merged_df = merged_df.merge(df, on=\"english\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5b503c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46947ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5391a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.drop_duplicates(subset=[\"english\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd80769c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a45b25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.sort_values(\"english\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b30cebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f71c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "alums = set()\n",
    "for col in merged_df.columns:\n",
    "    for i in range(len(merged_df)):\n",
    "        val = merged_df.iloc[i][col]\n",
    "        for j in str(val):\n",
    "            if not j.isalnum():\n",
    "                alums.add(j)\n",
    "\n",
    "len(alums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17f98e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv(\"data.csv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908a90cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum = 0\n",
    "cols = list(merged_df.columns)\n",
    "for _, rows in merged_df.iterrows():\n",
    "    for col in cols:\n",
    "        maximum = max( len(rows[col]), maximum)\n",
    "maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d30a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "LANGS = {\n",
    "    \"english\": \"en_XX\",\n",
    "    \"Portuguese\": \"pt_XX\",\n",
    "    \"Spanish\": \"es_XX\",\n",
    "    \"French\": \"fr_XX\",\n",
    "    \"German\": \"de_DE\",\n",
    "}\n",
    "\n",
    "pairs = []\n",
    "\n",
    "for _, row in merged_df.iterrows():\n",
    "    texts = row.to_dict()\n",
    "\n",
    "    # English ↔ others\n",
    "    for lang, code in LANGS.items():\n",
    "        if lang != \"english\":\n",
    "            pairs.append({\n",
    "                \"src_text\": texts[\"english\"],\n",
    "                \"tgt_text\": texts[lang],\n",
    "                \"src_lang\": \"en_XX\",\n",
    "                \"tgt_lang\": code,\n",
    "            })\n",
    "            pairs.append({\n",
    "                \"src_text\": texts[lang],\n",
    "                \"tgt_text\": texts[\"english\"],\n",
    "                \"src_lang\": code,\n",
    "                \"tgt_lang\": \"en_XX\",\n",
    "            })\n",
    "\n",
    "    # random cross-lingual pair\n",
    "    others = [l for l in LANGS if l != \"english\"]\n",
    "    l1, l2 = random.sample(others, 2)\n",
    "    pairs.append({\n",
    "        \"src_text\": texts[l1],\n",
    "        \"tgt_text\": texts[l2],\n",
    "        \"src_lang\": LANGS[l1],\n",
    "        \"tgt_lang\": LANGS[l2],\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fcb550",
   "metadata": {},
   "outputs": [],
   "source": [
    "from format_dataset import get_final_dataframe\n",
    "\n",
    "df = get_final_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b6bf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data2.csv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e9937f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data2.csv', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73f0f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffafb3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3691040",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenize_dataset import create_dataset_dict, map_dataset\n",
    "dataset = create_dataset_dict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0435554a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13688a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047ba0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d945d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenize_dataset import map_dataset\n",
    "tokenized_dataset = map_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56c6452",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 250\n",
    "# Preprocessing function\n",
    "def preprocess_function(examples):\n",
    "\n",
    "    inputs = examples[\"source\"]\n",
    "    targets = examples[\"target\"]\n",
    "    \n",
    "    # Tokenize inputs with source language\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "    \n",
    "    # labels[\"input_ids\"] = [\n",
    "    #     [(l if l != tokenizer.pad_token_id else -100) for l in label]\n",
    "    #     for label in labels[\"input_ids\"]\n",
    "    # ]\n",
    "\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352fc811",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenize_dataset import initlized_tokenizer\n",
    "tokenizer = initlized_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b41a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.src_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e838f4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tgt_lang = 'de_DE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43da2fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"Betriebsinhaber, die von dieser\"\n",
    "b = \"what is what the fuck\"\n",
    "tokenizer.decode(tokenizer.encode(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b826f888",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f11cbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff46d18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets =  dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64446f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from environment_variables import checkpoint\n",
    "tiktoken.get_encoding(\"mbart-large-50-many-to-many-mmt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70694e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "tokenized_datasets = load_from_disk('tokenized_dataset/French')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7cb608",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48db49cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07599d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3d050c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "small_dataset = DatasetDict({\n",
    "    \"train\": tokenized_datasets[\"train\"].select(range(10,000)),\n",
    "    \"val\": tokenized_datasets[\"val\"].select(range(100)),\n",
    "    \"test\": tokenized_datasets[\"test\"].select(range(100)),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ef7ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_dataset['train'][99] == tokenized_datasets['train'][99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8785729",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba063e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenized_datasets['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac328aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(10000 * 100 / 388317, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd773a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenized_datasets['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d962001d",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(2.58 * 48540 / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb286d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff211e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if a < 0:\n",
    "    print(\"Good\")\n",
    "else:\n",
    "    assert f\"What the fuck is going on\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5cc950",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7733632",
   "metadata": {},
   "source": [
    "## Version 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f6772b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('data/data.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2275ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3906fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = ['english', 'French', 'Portuguese', 'Spanish', 'German']\n",
    "lang_codes = {\n",
    "    \"english\": \"en_XX\",\n",
    "    \"Portuguese\": \"pt_XX\",\n",
    "    \"Spanish\": \"es_XX\",\n",
    "    \"French\": \"fr_XX\",\n",
    "    \"German\": \"de_DE\",\n",
    "} # Adjust codes as needed\n",
    "\n",
    "for i in langs:\n",
    "    data[i] = data[i].apply(lambda x : f\"<{lang_codes[i]}> {x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d25995",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585c9277",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1f5296",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataframe = pd.DataFrame(columns=['src_text', 'tgt_text'])\n",
    "final_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679cad11",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[0]['english']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6392ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "langs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f613d46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# for i in tqdm(range(len(data))):\n",
    "#         for src_lang in langs:\n",
    "#                 for tgt_lang in langs:\n",
    "#                         if src_lang != tgt_lang:\n",
    "#                                 final_dataframe['src_text'] = data.iloc[i][src_lang]\n",
    "#                                 final_dataframe['tgt_text'] = data.iloc[i][tgt_lang]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8659a313",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import random\n",
    "\n",
    "dataset = Dataset.from_pandas(data)\n",
    "# List of languages and their mBART codes\n",
    "\n",
    "\n",
    "def generate_pairs(example):\n",
    "    pairs = []\n",
    "    for src_lang in langs:\n",
    "        for tgt_lang in langs:\n",
    "            if src_lang != tgt_lang:\n",
    "                pairs.append({\n",
    "                        'src_text': example[src_lang],\n",
    "                        'tgt_text': example[tgt_lang]\n",
    "                })\n",
    "    # Optional: Shuffle and sample to avoid redundancy/excess data\n",
    "    random.shuffle(pairs)\n",
    "    return {'pairs': pairs[:10]}  # Limit per example if dataset is large\n",
    "\n",
    "# Flatten into a dataset of pairs\n",
    "dataset = dataset.map(generate_pairs, remove_columns=dataset.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d6d0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0009dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset['pairs'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99ae9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_generator():\n",
    "    for example in dataset:\n",
    "        for pair in example['pairs']:\n",
    "            yield pair\n",
    "\n",
    "dataset_final = Dataset.from_generator(pair_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46417e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_final = Dataset.from_list([item for sublist in dataset['pairs'] for item in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a808138",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b254577",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2ef6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/test\n",
    "dataset_final = dataset_final.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe78a40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_final.save_to_disk('data/final_dataset/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355ae0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_final['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecfa82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from peft import get_peft_model, LoraConfig\n",
    "\n",
    "model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Apply PEFT (LoRA config example; adjust ranks/modules as needed)\n",
    "peft_config = LoraConfig(\n",
    "    task_type=\"SEQ_2_SEQ_LM\",\n",
    "    r=8,  # LoRA rank\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]  # For mBART, focus on attention layers\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()  # Should show ~0.1-1% trainable params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4d318c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5ba66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.src_lang = \"en_XX\"\n",
    "tokenizer.tgt_lang = \"en_XX\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78210581",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 128  # Adjust as needed\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples['src_text']\n",
    "    targets = examples['tgt_text']\n",
    "    \n",
    "    # Tokenize sources (adds </s> at end; lang prefix becomes BOS ID)\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, \n",
    "        max_length=max_length, \n",
    "        truncation=True,\n",
    "        padding=\"max_length\"  # For batch consistency; data_collator can handle dynamic if preferred\n",
    "    )\n",
    "    \n",
    "    # Tokenize targets similarly\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets, \n",
    "            max_length=max_length, \n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = dataset_final.map(preprocess_function, batched=True, remove_columns=dataset_final.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49c9ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from environment_variables import raw_data_path\n",
    "dir_path = Path(raw_data_path)\n",
    "files = [f for f in dir_path.iterdir() if f.is_file()]\n",
    "files = sorted(files, key=lambda x : x.stat().st_size)\n",
    "files = [f.name.split('.')[0].split('-')[1] for f in files]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93444c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in files:\n",
    "    print(f.name, f.stat().st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f8cff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "merged_df = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aed158",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7173b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 01:48:05,894 | INFO | root | Creating dataframe for German\n",
      "2026-01-05 01:48:19,552 | INFO | root | Creating dataframe for French\n",
      "2026-01-05 01:48:34,381 | INFO | root | Creating dataframe for Spanish\n",
      "2026-01-05 01:48:49,287 | INFO | root | Creating dataframe for Portuguese\n"
     ]
    }
   ],
   "source": [
    "from format_dataset import get_final_dataframe\n",
    "\n",
    "dataframe = get_final_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12a52daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>German</th>\n",
       "      <th>French</th>\n",
       "      <th>Spanish</th>\n",
       "      <th>Portuguese</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"</td>\n",
       "      <td>\"</td>\n",
       "      <td>»</td>\n",
       "      <td>»</td>\n",
       "      <td>».</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>\" \"EEA\" shall mean the European Economic Area ...</td>\n",
       "      <td>\" \"EWR\": der Europäische Wirtschaftsraum im Si...</td>\n",
       "      <td>\"\"EEE\": l'Espace économique européen tel que d...</td>\n",
       "      <td>\"\"EEE\": el Espacio Económico Europeo según se ...</td>\n",
       "      <td>\"\"EEE\": o Espaço Económico Europeu, conforme d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>\" \"connected NCB\" shall mean an NCB real-time ...</td>\n",
       "      <td>\" \"angeschlossene NZB\": eine NZB, deren Echtze...</td>\n",
       "      <td>\"\"BCN connectée\": une BCN dont le système à rè...</td>\n",
       "      <td>\"\"BCN conectado\": el BCN cuyo sistema de liqui...</td>\n",
       "      <td>\"\"BCN ligado\": o sistema de liquidação por bru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>\" \"finality\" or \"final\" shall mean that the se...</td>\n",
       "      <td>\" \"Endgültigkeit\" bzw. \"endgültig\": Die Abwick...</td>\n",
       "      <td>\"\"caractère définitif\" ou \"définitif\": le fait...</td>\n",
       "      <td>\"\"firmeza\" o \"firme\": que la liquidación de un...</td>\n",
       "      <td>\"\"Carácter definitivo\" ou \"irrevogável\": signi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>\" \"inter-NCB accounts\" shall mean the accounts...</td>\n",
       "      <td>\" \"Inter-NZB-Konten\": die Verrechnungskonten, ...</td>\n",
       "      <td>\"\"comptes inter-BCN\": les comptes que les BCN ...</td>\n",
       "      <td>\"\"cuentas entre BCN\": las cuentas que cada BCN...</td>\n",
       "      <td>\"\"Contas inter-BCN\": as contas interbancárias ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               english  \\\n",
       "0                                                    \"   \n",
       "216  \" \"EEA\" shall mean the European Economic Area ...   \n",
       "217  \" \"connected NCB\" shall mean an NCB real-time ...   \n",
       "218  \" \"finality\" or \"final\" shall mean that the se...   \n",
       "219  \" \"inter-NCB accounts\" shall mean the accounts...   \n",
       "\n",
       "                                                German  \\\n",
       "0                                                    \"   \n",
       "216  \" \"EWR\": der Europäische Wirtschaftsraum im Si...   \n",
       "217  \" \"angeschlossene NZB\": eine NZB, deren Echtze...   \n",
       "218  \" \"Endgültigkeit\" bzw. \"endgültig\": Die Abwick...   \n",
       "219  \" \"Inter-NZB-Konten\": die Verrechnungskonten, ...   \n",
       "\n",
       "                                                French  \\\n",
       "0                                                    »   \n",
       "216  \"\"EEE\": l'Espace économique européen tel que d...   \n",
       "217  \"\"BCN connectée\": une BCN dont le système à rè...   \n",
       "218  \"\"caractère définitif\" ou \"définitif\": le fait...   \n",
       "219  \"\"comptes inter-BCN\": les comptes que les BCN ...   \n",
       "\n",
       "                                               Spanish  \\\n",
       "0                                                    »   \n",
       "216  \"\"EEE\": el Espacio Económico Europeo según se ...   \n",
       "217  \"\"BCN conectado\": el BCN cuyo sistema de liqui...   \n",
       "218  \"\"firmeza\" o \"firme\": que la liquidación de un...   \n",
       "219  \"\"cuentas entre BCN\": las cuentas que cada BCN...   \n",
       "\n",
       "                                            Portuguese  \n",
       "0                                                   ».  \n",
       "216  \"\"EEE\": o Espaço Económico Europeu, conforme d...  \n",
       "217  \"\"BCN ligado\": o sistema de liquidação por bru...  \n",
       "218  \"\"Carácter definitivo\" ou \"irrevogável\": signi...  \n",
       "219  \"\"Contas inter-BCN\": as contas interbancárias ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa394999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "363449"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ca5beb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.to_csv('data/csv_data/data3.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff34c2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataframe = pd.read_csv('./data/csv_data/data3.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc21fb66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>German</th>\n",
       "      <th>French</th>\n",
       "      <th>Spanish</th>\n",
       "      <th>Portuguese</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"</td>\n",
       "      <td>\"</td>\n",
       "      <td>»</td>\n",
       "      <td>»</td>\n",
       "      <td>».</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>\" \"EEA\" shall mean the European Economic Area ...</td>\n",
       "      <td>\" \"EWR\": der Europäische Wirtschaftsraum im Si...</td>\n",
       "      <td>\"\"EEE\": l'Espace économique européen tel que d...</td>\n",
       "      <td>\"\"EEE\": el Espacio Económico Europeo según se ...</td>\n",
       "      <td>\"\"EEE\": o Espaço Económico Europeu, conforme d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>\" \"connected NCB\" shall mean an NCB real-time ...</td>\n",
       "      <td>\" \"angeschlossene NZB\": eine NZB, deren Echtze...</td>\n",
       "      <td>\"\"BCN connectée\": une BCN dont le système à rè...</td>\n",
       "      <td>\"\"BCN conectado\": el BCN cuyo sistema de liqui...</td>\n",
       "      <td>\"\"BCN ligado\": o sistema de liquidação por bru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>\" \"finality\" or \"final\" shall mean that the se...</td>\n",
       "      <td>\" \"Endgültigkeit\" bzw. \"endgültig\": Die Abwick...</td>\n",
       "      <td>\"\"caractère définitif\" ou \"définitif\": le fait...</td>\n",
       "      <td>\"\"firmeza\" o \"firme\": que la liquidación de un...</td>\n",
       "      <td>\"\"Carácter definitivo\" ou \"irrevogável\": signi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>\" \"inter-NCB accounts\" shall mean the accounts...</td>\n",
       "      <td>\" \"Inter-NZB-Konten\": die Verrechnungskonten, ...</td>\n",
       "      <td>\"\"comptes inter-BCN\": les comptes que les BCN ...</td>\n",
       "      <td>\"\"cuentas entre BCN\": las cuentas que cada BCN...</td>\n",
       "      <td>\"\"Contas inter-BCN\": as contas interbancárias ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               english  \\\n",
       "0                                                    \"   \n",
       "216  \" \"EEA\" shall mean the European Economic Area ...   \n",
       "217  \" \"connected NCB\" shall mean an NCB real-time ...   \n",
       "218  \" \"finality\" or \"final\" shall mean that the se...   \n",
       "219  \" \"inter-NCB accounts\" shall mean the accounts...   \n",
       "\n",
       "                                                German  \\\n",
       "0                                                    \"   \n",
       "216  \" \"EWR\": der Europäische Wirtschaftsraum im Si...   \n",
       "217  \" \"angeschlossene NZB\": eine NZB, deren Echtze...   \n",
       "218  \" \"Endgültigkeit\" bzw. \"endgültig\": Die Abwick...   \n",
       "219  \" \"Inter-NZB-Konten\": die Verrechnungskonten, ...   \n",
       "\n",
       "                                                French  \\\n",
       "0                                                    »   \n",
       "216  \"\"EEE\": l'Espace économique européen tel que d...   \n",
       "217  \"\"BCN connectée\": une BCN dont le système à rè...   \n",
       "218  \"\"caractère définitif\" ou \"définitif\": le fait...   \n",
       "219  \"\"comptes inter-BCN\": les comptes que les BCN ...   \n",
       "\n",
       "                                               Spanish  \\\n",
       "0                                                    »   \n",
       "216  \"\"EEE\": el Espacio Económico Europeo según se ...   \n",
       "217  \"\"BCN conectado\": el BCN cuyo sistema de liqui...   \n",
       "218  \"\"firmeza\" o \"firme\": que la liquidación de un...   \n",
       "219  \"\"cuentas entre BCN\": las cuentas que cada BCN...   \n",
       "\n",
       "                                            Portuguese  \n",
       "0                                                   ».  \n",
       "216  \"\"EEE\": o Espaço Económico Europeu, conforme d...  \n",
       "217  \"\"BCN ligado\": o sistema de liquidação por bru...  \n",
       "218  \"\"Carácter definitivo\" ou \"irrevogável\": signi...  \n",
       "219  \"\"Contas inter-BCN\": as contas interbancárias ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2430b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 01:52:45,664 | INFO | datasets | PyTorch version 2.5.1+cu124 available.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ca70b10f51040f28d7538969c0b7549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/363449 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c1a9c80dc76413583fa7ecb53eed2cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tokenize_dataset import create_dataset_dict\n",
    "\n",
    "final_dataset = create_dataset_dict(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06d832eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['source', 'src_lang', 'target', 'tgt_lang'],\n",
       "        num_rows: 2907592\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['source', 'src_lang', 'target', 'tgt_lang'],\n",
       "        num_rows: 363449\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['source', 'src_lang', 'target', 'tgt_lang'],\n",
       "        num_rows: 363449\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2bc8430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('src Obvious formal errors such as typing errors on a proof of origin should '\n",
      " 'not cause the document to be rejected if those errors are not such as to '\n",
      " 'create doubts concerning the correctness of the statements made in the '\n",
      " 'document.')\n",
      "('tgt Los errores de forma manifiestos, tales como los errores de mecanografía '\n",
      " 'en un documento de prueba del origen no implicarán el rechazo del documento '\n",
      " 'si dichos errores no pueden suscitar dudas acerca de la exactitud de las '\n",
      " 'declaraciones contenidas en dicho documento.')\n",
      "'src lang en_XX'\n",
      "'tgt lang es_XX'\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "a = 56454\n",
    "pprint(\"src \" + final_dataset['test']['source'][a])\n",
    "pprint(\"tgt \" + final_dataset['test']['target'][a])\n",
    "pprint(\"src lang \" + final_dataset['test']['src_lang'][a])\n",
    "pprint(\"tgt lang \" + final_dataset['test']['tgt_lang'][a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2831e2d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ef6ece06f3b498f89948b80ee9a9f1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/3 shards):   0%|          | 0/2907592 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7873081cb794fa6a2d3388f7f7f660a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/363449 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9894f0b3b9394400993cd44acb54bf8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/363449 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_dataset.save_to_disk('./temp_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed57b605",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "final_dataset = load_from_disk('./temp_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57564ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['source', 'src_lang', 'target', 'tgt_lang'],\n",
       "        num_rows: 2907592\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['source', 'src_lang', 'target', 'tgt_lang'],\n",
       "        num_rows: 363449\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['source', 'src_lang', 'target', 'tgt_lang'],\n",
       "        num_rows: 363449\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "741cf8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenize_dataset import map_dataset\n",
    "from initialize_model import init_tokenizer\n",
    "tokenizer = init_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4b6efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31e0ce952cf5411fa97476ae22bf8ff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2907592 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Machine Learning\\Large language Models\\LLM\\LLMenv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92a2d32130bf4fcf9340228acf913292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/363449 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa6cec192dfd40a680bf1a3da20d033b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/363449 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = map_dataset(tokenizer=tokenizer, dataset=final_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342c19cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2907592\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 363449\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 363449\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b28f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e0d249f45e64bd8834d07fdc2a526f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/19 shards):   0%|          | 0/2907592 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f858d40cebf24fe5a3bf72075f6b66e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/3 shards):   0%|          | 0/363449 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c96aa876bc043b9a3fd41ea0d9e3a97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/3 shards):   0%|          | 0/363449 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from environment_variables import tokenized_data_path\n",
    "tokenized_dataset.save_to_disk(tokenized_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d41cf72a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "439adf30d97c45c29572bd5204c2bfbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from environment_variables import tokenized_data_path\n",
    "tokenized_dataset = load_from_disk(tokenized_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbaf4f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "a = 56454\n",
    "src = tokenized_dataset['test']['input_ids'][a]\n",
    "tgt = tokenized_dataset['test']['labels'][a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e42499",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
