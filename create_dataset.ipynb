{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b43660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lxml import etree\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1492f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2758d3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = Path(\"data/\")\n",
    "\n",
    "files = [f.name.split('.')[0] for f in dir_path.iterdir() if f.is_file()]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21765bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(file_name):\n",
    "    # Path to TMX file\n",
    "    tmx_path = f\"data/{file_name}.tmx\"\n",
    "\n",
    "    # Parse the TMX XML\n",
    "    tree = etree.parse(tmx_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    rows = []\n",
    "    name = file_name.split('.')[0].split('-')[1]\n",
    "\n",
    "    if name == \"pt\":\n",
    "        lang_name = \"Portuguese\"\n",
    "    elif name == \"fr\":\n",
    "        lang_name = \"French\"\n",
    "    elif name == \"es\":\n",
    "        lang_name = \"Spanish\"\n",
    "    else :\n",
    "        lang_name = \"German\"\n",
    "    \n",
    "    print(f\"Creating the dataframe for {lang_name} language\")\n",
    "    # TMX content is inside <body>\n",
    "    body = root.find(\"body\")\n",
    "\n",
    "    for tu in body.findall(\"tu\"):\n",
    "        en_text = None\n",
    "        second_text = None\n",
    "\n",
    "        for tuv in tu.findall(\"tuv\"):\n",
    "            # Language attribute (xml:lang or lang)\n",
    "            lang = (\n",
    "                tuv.attrib.get(\"{http://www.w3.org/XML/1998/namespace}lang\")\n",
    "                or tuv.attrib.get(\"lang\")\n",
    "            )\n",
    "            seg = tuv.find(\"seg\")\n",
    "            if seg is None:\n",
    "                continue\n",
    "\n",
    "            if lang == \"en\":\n",
    "                en_text = seg.text\n",
    "            elif lang == name:\n",
    "                second_text = seg.text\n",
    "\n",
    "        # Only keep pairs where both languages exist\n",
    "        if en_text and second_text:\n",
    "            rows.append({\n",
    "                \"english\": en_text,\n",
    "                lang_name : second_text\n",
    "            })\n",
    "\n",
    "    # Create DataFrame\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9899d4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "for f in files[::-1]:\n",
    "    print(f\"Starting for langauage {f}\", flush=True)\n",
    "    d = get_data(f)\n",
    "    data[f] = d\n",
    "    ## df3 = df2.merge(df, on='english', how='inner')\n",
    "    print(f\"Done with the language {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cafaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "\n",
    "    # Replace non-breaking space\n",
    "    text = text.replace('\\xa0', ' ')\n",
    "\n",
    "    # Normalize dashes\n",
    "    text = re.sub(r'[‐-–—]', '-', text)\n",
    "\n",
    "    # Normalize quotes\n",
    "    text = text.replace('“', '\"').replace('”', '\"')\n",
    "    text = text.replace('‘', \"'\").replace('’', \"'\")\n",
    "\n",
    "    # Remove replacement character\n",
    "    text = text.replace('�', '')\n",
    "\n",
    "    # Collapse whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50b9f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data:\n",
    "    print(f\"starting for {i}\")\n",
    "    data[i] = data[i].map(clean_text)\n",
    "    data[i] = data[i].sort_values(\"english\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eabb683",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in data.items():\n",
    "    print(f\"The lenght of langauge {key} is {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5994dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df = min(data.values(), key=len)\n",
    "len(base_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02e9edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = base_df.merge(data['en-pt'], how='inner', on='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e19b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df3.drop_duplicates(subset=[\"english\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54990ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.loc[454]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe295b23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fc1492",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df = min(data.values(), key=len)\n",
    "len(base_df)\n",
    "\n",
    "merged_df = base_df.copy()\n",
    "\n",
    "for df in data.values():\n",
    "    if df is not base_df:\n",
    "        merged_df = merged_df.merge(df, on=\"english\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5b503c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46947ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5391a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.drop_duplicates(subset=[\"english\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd80769c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a45b25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.sort_values(\"english\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b30cebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f71c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "alums = set()\n",
    "for col in merged_df.columns:\n",
    "    for i in range(len(merged_df)):\n",
    "        val = merged_df.iloc[i][col]\n",
    "        for j in str(val):\n",
    "            if not j.isalnum():\n",
    "                alums.add(j)\n",
    "\n",
    "len(alums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17f98e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv(\"data.csv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908a90cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum = 0\n",
    "cols = list(merged_df.columns)\n",
    "for _, rows in merged_df.iterrows():\n",
    "    for col in cols:\n",
    "        maximum = max( len(rows[col]), maximum)\n",
    "maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d30a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "LANGS = {\n",
    "    \"english\": \"en_XX\",\n",
    "    \"Portuguese\": \"pt_XX\",\n",
    "    \"Spanish\": \"es_XX\",\n",
    "    \"French\": \"fr_XX\",\n",
    "    \"German\": \"de_DE\",\n",
    "}\n",
    "\n",
    "pairs = []\n",
    "\n",
    "for _, row in merged_df.iterrows():\n",
    "    texts = row.to_dict()\n",
    "\n",
    "    # English ↔ others\n",
    "    for lang, code in LANGS.items():\n",
    "        if lang != \"english\":\n",
    "            pairs.append({\n",
    "                \"src_text\": texts[\"english\"],\n",
    "                \"tgt_text\": texts[lang],\n",
    "                \"src_lang\": \"en_XX\",\n",
    "                \"tgt_lang\": code,\n",
    "            })\n",
    "            pairs.append({\n",
    "                \"src_text\": texts[lang],\n",
    "                \"tgt_text\": texts[\"english\"],\n",
    "                \"src_lang\": code,\n",
    "                \"tgt_lang\": \"en_XX\",\n",
    "            })\n",
    "\n",
    "    # random cross-lingual pair\n",
    "    others = [l for l in LANGS if l != \"english\"]\n",
    "    l1, l2 = random.sample(others, 2)\n",
    "    pairs.append({\n",
    "        \"src_text\": texts[l1],\n",
    "        \"tgt_text\": texts[l2],\n",
    "        \"src_lang\": LANGS[l1],\n",
    "        \"tgt_lang\": LANGS[l2],\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fcb550",
   "metadata": {},
   "outputs": [],
   "source": [
    "from format_dataset import get_final_dataframe\n",
    "\n",
    "df = get_final_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b6bf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data2.csv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e9937f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data2.csv', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73f0f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffafb3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3691040",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenize_dataset import create_dataset_dict, map_dataset\n",
    "dataset = create_dataset_dict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0435554a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13688a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047ba0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d945d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenize_dataset import map_dataset\n",
    "tokenized_dataset = map_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56c6452",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 250\n",
    "# Preprocessing function\n",
    "def preprocess_function(examples):\n",
    "\n",
    "    inputs = examples[\"source\"]\n",
    "    targets = examples[\"target\"]\n",
    "    \n",
    "    # Tokenize inputs with source language\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "    \n",
    "    # labels[\"input_ids\"] = [\n",
    "    #     [(l if l != tokenizer.pad_token_id else -100) for l in label]\n",
    "    #     for label in labels[\"input_ids\"]\n",
    "    # ]\n",
    "\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352fc811",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenize_dataset import initlized_tokenizer\n",
    "tokenizer = initlized_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b41a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.src_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e838f4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tgt_lang = 'de_DE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43da2fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"Betriebsinhaber, die von dieser\"\n",
    "b = \"what is what the fuck\"\n",
    "tokenizer.decode(tokenizer.encode(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b826f888",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f11cbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff46d18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets =  dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64446f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from environment_variables import checkpoint\n",
    "tiktoken.get_encoding(\"mbart-large-50-many-to-many-mmt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70694e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "tokenized_datasets = load_from_disk('tokenized_dataset/French')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7cb608",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48db49cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07599d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3d050c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "small_dataset = DatasetDict({\n",
    "    \"train\": tokenized_datasets[\"train\"].select(range(10,000)),\n",
    "    \"val\": tokenized_datasets[\"val\"].select(range(100)),\n",
    "    \"test\": tokenized_datasets[\"test\"].select(range(100)),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ef7ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_dataset['train'][99] == tokenized_datasets['train'][99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8785729",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba063e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenized_datasets['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac328aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(10000 * 100 / 388317, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd773a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenized_datasets['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d962001d",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(2.58 * 48540 / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb286d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff211e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if a < 0:\n",
    "    print(\"Good\")\n",
    "else:\n",
    "    assert f\"What the fuck is going on\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5cc950",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7733632",
   "metadata": {},
   "source": [
    "## Version 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f6772b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('data/data.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2275ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3906fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = ['english', 'French', 'Portuguese', 'Spanish', 'German']\n",
    "lang_codes = {\n",
    "    \"english\": \"en_XX\",\n",
    "    \"Portuguese\": \"pt_XX\",\n",
    "    \"Spanish\": \"es_XX\",\n",
    "    \"French\": \"fr_XX\",\n",
    "    \"German\": \"de_DE\",\n",
    "} # Adjust codes as needed\n",
    "\n",
    "for i in langs:\n",
    "    data[i] = data[i].apply(lambda x : f\"<{lang_codes[i]}> {x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d25995",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585c9277",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1f5296",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataframe = pd.DataFrame(columns=['src_text', 'tgt_text'])\n",
    "final_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679cad11",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[0]['english']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6392ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "langs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f613d46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# for i in tqdm(range(len(data))):\n",
    "#         for src_lang in langs:\n",
    "#                 for tgt_lang in langs:\n",
    "#                         if src_lang != tgt_lang:\n",
    "#                                 final_dataframe['src_text'] = data.iloc[i][src_lang]\n",
    "#                                 final_dataframe['tgt_text'] = data.iloc[i][tgt_lang]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8659a313",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import random\n",
    "\n",
    "dataset = Dataset.from_pandas(data)\n",
    "# List of languages and their mBART codes\n",
    "\n",
    "\n",
    "def generate_pairs(example):\n",
    "    pairs = []\n",
    "    for src_lang in langs:\n",
    "        for tgt_lang in langs:\n",
    "            if src_lang != tgt_lang:\n",
    "                pairs.append({\n",
    "                        'src_text': example[src_lang],\n",
    "                        'tgt_text': example[tgt_lang]\n",
    "                })\n",
    "    # Optional: Shuffle and sample to avoid redundancy/excess data\n",
    "    random.shuffle(pairs)\n",
    "    return {'pairs': pairs[:10]}  # Limit per example if dataset is large\n",
    "\n",
    "# Flatten into a dataset of pairs\n",
    "dataset = dataset.map(generate_pairs, remove_columns=dataset.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d6d0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0009dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset['pairs'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99ae9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_generator():\n",
    "    for example in dataset:\n",
    "        for pair in example['pairs']:\n",
    "            yield pair\n",
    "\n",
    "dataset_final = Dataset.from_generator(pair_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46417e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_final = Dataset.from_list([item for sublist in dataset['pairs'] for item in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a808138",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b254577",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2ef6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/test\n",
    "dataset_final = dataset_final.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe78a40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_final.save_to_disk('data/final_dataset/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355ae0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_final['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecfa82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from peft import get_peft_model, LoraConfig\n",
    "\n",
    "model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Apply PEFT (LoRA config example; adjust ranks/modules as needed)\n",
    "peft_config = LoraConfig(\n",
    "    task_type=\"SEQ_2_SEQ_LM\",\n",
    "    r=8,  # LoRA rank\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]  # For mBART, focus on attention layers\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()  # Should show ~0.1-1% trainable params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4d318c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5ba66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.src_lang = \"en_XX\"\n",
    "tokenizer.tgt_lang = \"en_XX\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78210581",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 128  # Adjust as needed\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples['src_text']\n",
    "    targets = examples['tgt_text']\n",
    "    \n",
    "    # Tokenize sources (adds </s> at end; lang prefix becomes BOS ID)\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, \n",
    "        max_length=max_length, \n",
    "        truncation=True,\n",
    "        padding=\"max_length\"  # For batch consistency; data_collator can handle dynamic if preferred\n",
    "    )\n",
    "    \n",
    "    # Tokenize targets similarly\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets, \n",
    "            max_length=max_length, \n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = dataset_final.map(preprocess_function, batched=True, remove_columns=dataset_final.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49c9ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from environment_variables import raw_data_path\n",
    "dir_path = Path(raw_data_path)\n",
    "files = [f for f in dir_path.iterdir() if f.is_file()]\n",
    "files = sorted(files, key=lambda x : x.stat().st_size)\n",
    "files = [f.name.split('.')[0].split('-')[1] for f in files]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93444c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in files:\n",
    "    print(f.name, f.stat().st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f8cff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "merged_df = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aed158",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7173b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 01:48:05,894 | INFO | root | Creating dataframe for German\n",
      "2026-01-05 01:48:19,552 | INFO | root | Creating dataframe for French\n",
      "2026-01-05 01:48:34,381 | INFO | root | Creating dataframe for Spanish\n",
      "2026-01-05 01:48:49,287 | INFO | root | Creating dataframe for Portuguese\n"
     ]
    }
   ],
   "source": [
    "from format_dataset import get_final_dataframe\n",
    "\n",
    "dataframe = get_final_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12a52daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>German</th>\n",
       "      <th>French</th>\n",
       "      <th>Spanish</th>\n",
       "      <th>Portuguese</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"</td>\n",
       "      <td>\"</td>\n",
       "      <td>»</td>\n",
       "      <td>»</td>\n",
       "      <td>».</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>\" \"EEA\" shall mean the European Economic Area ...</td>\n",
       "      <td>\" \"EWR\": der Europäische Wirtschaftsraum im Si...</td>\n",
       "      <td>\"\"EEE\": l'Espace économique européen tel que d...</td>\n",
       "      <td>\"\"EEE\": el Espacio Económico Europeo según se ...</td>\n",
       "      <td>\"\"EEE\": o Espaço Económico Europeu, conforme d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>\" \"connected NCB\" shall mean an NCB real-time ...</td>\n",
       "      <td>\" \"angeschlossene NZB\": eine NZB, deren Echtze...</td>\n",
       "      <td>\"\"BCN connectée\": une BCN dont le système à rè...</td>\n",
       "      <td>\"\"BCN conectado\": el BCN cuyo sistema de liqui...</td>\n",
       "      <td>\"\"BCN ligado\": o sistema de liquidação por bru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>\" \"finality\" or \"final\" shall mean that the se...</td>\n",
       "      <td>\" \"Endgültigkeit\" bzw. \"endgültig\": Die Abwick...</td>\n",
       "      <td>\"\"caractère définitif\" ou \"définitif\": le fait...</td>\n",
       "      <td>\"\"firmeza\" o \"firme\": que la liquidación de un...</td>\n",
       "      <td>\"\"Carácter definitivo\" ou \"irrevogável\": signi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>\" \"inter-NCB accounts\" shall mean the accounts...</td>\n",
       "      <td>\" \"Inter-NZB-Konten\": die Verrechnungskonten, ...</td>\n",
       "      <td>\"\"comptes inter-BCN\": les comptes que les BCN ...</td>\n",
       "      <td>\"\"cuentas entre BCN\": las cuentas que cada BCN...</td>\n",
       "      <td>\"\"Contas inter-BCN\": as contas interbancárias ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               english  \\\n",
       "0                                                    \"   \n",
       "216  \" \"EEA\" shall mean the European Economic Area ...   \n",
       "217  \" \"connected NCB\" shall mean an NCB real-time ...   \n",
       "218  \" \"finality\" or \"final\" shall mean that the se...   \n",
       "219  \" \"inter-NCB accounts\" shall mean the accounts...   \n",
       "\n",
       "                                                German  \\\n",
       "0                                                    \"   \n",
       "216  \" \"EWR\": der Europäische Wirtschaftsraum im Si...   \n",
       "217  \" \"angeschlossene NZB\": eine NZB, deren Echtze...   \n",
       "218  \" \"Endgültigkeit\" bzw. \"endgültig\": Die Abwick...   \n",
       "219  \" \"Inter-NZB-Konten\": die Verrechnungskonten, ...   \n",
       "\n",
       "                                                French  \\\n",
       "0                                                    »   \n",
       "216  \"\"EEE\": l'Espace économique européen tel que d...   \n",
       "217  \"\"BCN connectée\": une BCN dont le système à rè...   \n",
       "218  \"\"caractère définitif\" ou \"définitif\": le fait...   \n",
       "219  \"\"comptes inter-BCN\": les comptes que les BCN ...   \n",
       "\n",
       "                                               Spanish  \\\n",
       "0                                                    »   \n",
       "216  \"\"EEE\": el Espacio Económico Europeo según se ...   \n",
       "217  \"\"BCN conectado\": el BCN cuyo sistema de liqui...   \n",
       "218  \"\"firmeza\" o \"firme\": que la liquidación de un...   \n",
       "219  \"\"cuentas entre BCN\": las cuentas que cada BCN...   \n",
       "\n",
       "                                            Portuguese  \n",
       "0                                                   ».  \n",
       "216  \"\"EEE\": o Espaço Económico Europeu, conforme d...  \n",
       "217  \"\"BCN ligado\": o sistema de liquidação por bru...  \n",
       "218  \"\"Carácter definitivo\" ou \"irrevogável\": signi...  \n",
       "219  \"\"Contas inter-BCN\": as contas interbancárias ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa394999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "363449"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ca5beb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.to_csv('data/csv_data/data3.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff34c2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataframe = pd.read_csv('./data/csv_data/data3.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc21fb66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>German</th>\n",
       "      <th>French</th>\n",
       "      <th>Spanish</th>\n",
       "      <th>Portuguese</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"</td>\n",
       "      <td>\"</td>\n",
       "      <td>»</td>\n",
       "      <td>»</td>\n",
       "      <td>».</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>\" \"EEA\" shall mean the European Economic Area ...</td>\n",
       "      <td>\" \"EWR\": der Europäische Wirtschaftsraum im Si...</td>\n",
       "      <td>\"\"EEE\": l'Espace économique européen tel que d...</td>\n",
       "      <td>\"\"EEE\": el Espacio Económico Europeo según se ...</td>\n",
       "      <td>\"\"EEE\": o Espaço Económico Europeu, conforme d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>\" \"connected NCB\" shall mean an NCB real-time ...</td>\n",
       "      <td>\" \"angeschlossene NZB\": eine NZB, deren Echtze...</td>\n",
       "      <td>\"\"BCN connectée\": une BCN dont le système à rè...</td>\n",
       "      <td>\"\"BCN conectado\": el BCN cuyo sistema de liqui...</td>\n",
       "      <td>\"\"BCN ligado\": o sistema de liquidação por bru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>\" \"finality\" or \"final\" shall mean that the se...</td>\n",
       "      <td>\" \"Endgültigkeit\" bzw. \"endgültig\": Die Abwick...</td>\n",
       "      <td>\"\"caractère définitif\" ou \"définitif\": le fait...</td>\n",
       "      <td>\"\"firmeza\" o \"firme\": que la liquidación de un...</td>\n",
       "      <td>\"\"Carácter definitivo\" ou \"irrevogável\": signi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>\" \"inter-NCB accounts\" shall mean the accounts...</td>\n",
       "      <td>\" \"Inter-NZB-Konten\": die Verrechnungskonten, ...</td>\n",
       "      <td>\"\"comptes inter-BCN\": les comptes que les BCN ...</td>\n",
       "      <td>\"\"cuentas entre BCN\": las cuentas que cada BCN...</td>\n",
       "      <td>\"\"Contas inter-BCN\": as contas interbancárias ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               english  \\\n",
       "0                                                    \"   \n",
       "216  \" \"EEA\" shall mean the European Economic Area ...   \n",
       "217  \" \"connected NCB\" shall mean an NCB real-time ...   \n",
       "218  \" \"finality\" or \"final\" shall mean that the se...   \n",
       "219  \" \"inter-NCB accounts\" shall mean the accounts...   \n",
       "\n",
       "                                                German  \\\n",
       "0                                                    \"   \n",
       "216  \" \"EWR\": der Europäische Wirtschaftsraum im Si...   \n",
       "217  \" \"angeschlossene NZB\": eine NZB, deren Echtze...   \n",
       "218  \" \"Endgültigkeit\" bzw. \"endgültig\": Die Abwick...   \n",
       "219  \" \"Inter-NZB-Konten\": die Verrechnungskonten, ...   \n",
       "\n",
       "                                                French  \\\n",
       "0                                                    »   \n",
       "216  \"\"EEE\": l'Espace économique européen tel que d...   \n",
       "217  \"\"BCN connectée\": une BCN dont le système à rè...   \n",
       "218  \"\"caractère définitif\" ou \"définitif\": le fait...   \n",
       "219  \"\"comptes inter-BCN\": les comptes que les BCN ...   \n",
       "\n",
       "                                               Spanish  \\\n",
       "0                                                    »   \n",
       "216  \"\"EEE\": el Espacio Económico Europeo según se ...   \n",
       "217  \"\"BCN conectado\": el BCN cuyo sistema de liqui...   \n",
       "218  \"\"firmeza\" o \"firme\": que la liquidación de un...   \n",
       "219  \"\"cuentas entre BCN\": las cuentas que cada BCN...   \n",
       "\n",
       "                                            Portuguese  \n",
       "0                                                   ».  \n",
       "216  \"\"EEE\": o Espaço Económico Europeu, conforme d...  \n",
       "217  \"\"BCN ligado\": o sistema de liquidação por bru...  \n",
       "218  \"\"Carácter definitivo\" ou \"irrevogável\": signi...  \n",
       "219  \"\"Contas inter-BCN\": as contas interbancárias ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2430b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 01:52:45,664 | INFO | datasets | PyTorch version 2.5.1+cu124 available.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ca70b10f51040f28d7538969c0b7549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/363449 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c1a9c80dc76413583fa7ecb53eed2cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tokenize_dataset import create_dataset_dict\n",
    "\n",
    "final_dataset = create_dataset_dict(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06d832eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['source', 'src_lang', 'target', 'tgt_lang'],\n",
       "        num_rows: 2907592\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['source', 'src_lang', 'target', 'tgt_lang'],\n",
       "        num_rows: 363449\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['source', 'src_lang', 'target', 'tgt_lang'],\n",
       "        num_rows: 363449\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2bc8430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('src Obvious formal errors such as typing errors on a proof of origin should '\n",
      " 'not cause the document to be rejected if those errors are not such as to '\n",
      " 'create doubts concerning the correctness of the statements made in the '\n",
      " 'document.')\n",
      "('tgt Los errores de forma manifiestos, tales como los errores de mecanografía '\n",
      " 'en un documento de prueba del origen no implicarán el rechazo del documento '\n",
      " 'si dichos errores no pueden suscitar dudas acerca de la exactitud de las '\n",
      " 'declaraciones contenidas en dicho documento.')\n",
      "'src lang en_XX'\n",
      "'tgt lang es_XX'\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "a = 56454\n",
    "pprint(\"src \" + final_dataset['test']['source'][a])\n",
    "pprint(\"tgt \" + final_dataset['test']['target'][a])\n",
    "pprint(\"src lang \" + final_dataset['test']['src_lang'][a])\n",
    "pprint(\"tgt lang \" + final_dataset['test']['tgt_lang'][a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2831e2d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ef6ece06f3b498f89948b80ee9a9f1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/3 shards):   0%|          | 0/2907592 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7873081cb794fa6a2d3388f7f7f660a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/363449 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9894f0b3b9394400993cd44acb54bf8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/363449 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_dataset.save_to_disk('./temp_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed57b605",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "final_dataset = load_from_disk('./temp_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57564ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['source', 'src_lang', 'target', 'tgt_lang'],\n",
       "        num_rows: 2907592\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['source', 'src_lang', 'target', 'tgt_lang'],\n",
       "        num_rows: 363449\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['source', 'src_lang', 'target', 'tgt_lang'],\n",
       "        num_rows: 363449\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "741cf8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenize_dataset import map_dataset\n",
    "from initialize_model import init_tokenizer\n",
    "tokenizer = init_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4b6efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31e0ce952cf5411fa97476ae22bf8ff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2907592 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Machine Learning\\Large language Models\\LLM\\LLMenv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92a2d32130bf4fcf9340228acf913292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/363449 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa6cec192dfd40a680bf1a3da20d033b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/363449 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = map_dataset(tokenizer=tokenizer, dataset=final_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342c19cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2907592\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 363449\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 363449\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b28f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e0d249f45e64bd8834d07fdc2a526f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/19 shards):   0%|          | 0/2907592 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f858d40cebf24fe5a3bf72075f6b66e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/3 shards):   0%|          | 0/363449 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c96aa876bc043b9a3fd41ea0d9e3a97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/3 shards):   0%|          | 0/363449 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from environment_variables import tokenized_data_path\n",
    "tokenized_dataset.save_to_disk(tokenized_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d41cf72a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "439adf30d97c45c29572bd5204c2bfbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from environment_variables import tokenized_data_path\n",
    "tokenized_dataset = load_from_disk(tokenized_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbaf4f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "a = 56454\n",
    "src = tokenized_dataset['test']['input_ids'][a]\n",
    "tgt = tokenized_dataset['test']['labels'][a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05e42499",
   "metadata": {},
   "outputs": [],
   "source": [
    "from initialize_model import init_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "185557af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-08 15:46:05,482 | INFO | accelerate.utils.modeling | We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    }
   ],
   "source": [
    "model = init_model(get_lora_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5faeab87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vllm in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (0.13.0)\n",
      "Requirement already satisfied: regex in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from vllm) (2024.11.6)\n",
      "Requirement already satisfied: cachetools in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from vllm) (6.1.0)\n",
      "Requirement already satisfied: psutil in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from vllm) (6.1.1)\n",
      "Requirement already satisfied: sentencepiece in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from vllm) (0.2.0)\n",
      "Requirement already satisfied: numpy in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from vllm) (2.2.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from vllm) (2.32.3)\n",
      "Requirement already satisfied: tqdm in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from vllm) (4.67.1)\n",
      "Requirement already satisfied: blake3 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from vllm) (1.0.8)\n",
      "Requirement already satisfied: py-cpuinfo in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from vllm) (9.0.0)\n",
      "Requirement already satisfied: transformers<5,>=4.56.0 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from vllm) (4.57.3)\n",
      "Requirement already satisfied: tokenizers>=0.21.1 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from vllm) (0.22.1)\n",
      "Requirement already satisfied: protobuf in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from vllm) (5.29.2)\n",
      "Requirement already satisfied: fastapi>=0.115.0 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from fastapi[standard]>=0.115.0->vllm) (0.128.0)\n",
      "Requirement already satisfied: aiohttp in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from vllm) (3.11.11)\n",
      "Requirement already satisfied: openai>=1.99.1 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from vllm) (2.14.0)\n",
      "Requirement already satisfied: pydantic>=2.12.0 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from vllm) (2.12.5)\n",
      "Requirement already satisfied: prometheus_client>=0.18.0 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from vllm) (0.21.1)\n",
      "Requirement already satisfied: pillow in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from vllm) (12.1.0)\n",
      "Requirement already satisfied: prometheus-fastapi-instrumentator>=7.0.0 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from vllm) (7.1.0)\n",
      "Requirement already satisfied: tiktoken>=0.6.0 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from vllm) (0.12.0)\n",
      "Requirement already satisfied: lm-format-enforcer==0.11.3 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from vllm) (0.11.3)\n",
      "Requirement already satisfied: outlines_core==0.2.11 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from vllm) (0.2.11)\n",
      "Requirement already satisfied: diskcache==5.6.3 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from vllm) (5.6.3)\n",
      "Requirement already satisfied: lark==1.2.2 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from vllm) (1.2.2)\n",
      "Requirement already satisfied: typing_extensions>=4.10 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from vllm) (4.15.0)\n",
      "Requirement already satisfied: filelock>=3.16.1 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from vllm) (3.20.2)\n",
      "Requirement already satisfied: partial-json-parser in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from vllm) (0.2.1.1.post7)\n",
      "Requirement already satisfied: pyzmq>=25.0.0 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from vllm) (26.2.0)\n",
      "Requirement already satisfied: msgspec in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from vllm) (0.20.0)\n",
      "Requirement already satisfied: gguf>=0.17.0 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from vllm) (0.17.1)\n",
      "Requirement already satisfied: mistral_common>=1.8.5 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from mistral_common[image]>=1.8.5->vllm) (1.8.8)\n",
      "Requirement already satisfied: opencv-python-headless>=4.11.0 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from vllm) (4.12.0.88)\n",
      "Requirement already satisfied: pyyaml in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from vllm) (6.0.2)\n",
      "Requirement already satisfied: einops in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from vllm) (0.8.1)\n",
      "Requirement already satisfied: compressed-tensors==0.12.2 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from vllm) (0.12.2)\n",
      "Requirement already satisfied: depyf==0.20.0 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from vllm) (0.20.0)\n",
      "Requirement already satisfied: cloudpickle in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from vllm) (3.1.2)\n",
      "Requirement already satisfied: watchfiles in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from vllm) (1.1.1)\n",
      "Requirement already satisfied: python-json-logger in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from vllm) (3.2.1)\n",
      "Requirement already satisfied: scipy in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from vllm) (1.16.3)\n",
      "Requirement already satisfied: ninja in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from vllm) (1.13.0)\n",
      "Requirement already satisfied: pybase64 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from vllm) (1.4.3)\n",
      "Requirement already satisfied: cbor2 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from vllm) (5.8.0)\n",
      "Requirement already satisfied: ijson in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from vllm) (3.4.0.post0)\n",
      "Requirement already satisfied: setproctitle in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from vllm) (1.3.4)\n",
      "Requirement already satisfied: openai-harmony>=0.0.3 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from vllm) (0.0.8)\n",
      "Requirement already satisfied: anthropic==0.71.0 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from vllm) (0.71.0)\n",
      "Requirement already satisfied: model-hosting-container-standards<1.0.0,>=0.1.9 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from vllm) (0.1.12)\n",
      "Requirement already satisfied: mcp in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from vllm) (1.25.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from anthropic==0.71.0->vllm) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from anthropic==0.71.0->vllm) (1.9.0)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.15 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from anthropic==0.71.0->vllm) (0.17.0)\n",
      "Requirement already satisfied: httpx<1,>=0.25.0 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from anthropic==0.71.0->vllm) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from anthropic==0.71.0->vllm) (0.12.0)\n",
      "Requirement already satisfied: sniffio in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from anthropic==0.71.0->vllm) (1.3.1)\n",
      "Requirement already satisfied: torch>=1.7.0 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from compressed-tensors==0.12.2->vllm) (2.5.1+cu124)\n",
      "Requirement already satisfied: loguru in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from compressed-tensors==0.12.2->vllm) (0.7.3)\n",
      "Requirement already satisfied: astor in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from depyf==0.20.0->vllm) (0.8.1)\n",
      "Requirement already satisfied: dill in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from depyf==0.20.0->vllm) (0.3.8)\n",
      "Requirement already satisfied: interegular>=0.3.2 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from lm-format-enforcer==0.11.3->vllm) (0.3.3)\n",
      "Requirement already satisfied: packaging in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from lm-format-enforcer==0.11.3->vllm) (24.2)\n",
      "Requirement already satisfied: starlette<0.51.0,>=0.40.0 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (0.50.0)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (0.0.4)\n",
      "Requirement already satisfied: fastapi-cli>=0.0.8 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.0.20)\n",
      "Requirement already satisfied: jinja2>=3.1.5 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from fastapi[standard]>=0.115.0->vllm) (3.1.5)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from fastapi[standard]>=0.115.0->vllm) (0.0.21)\n",
      "Requirement already satisfied: email-validator>=2.0.0 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from fastapi[standard]>=0.115.0->vllm) (2.3.0)\n",
      "Requirement already satisfied: uvicorn>=0.12.0 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.35.0)\n",
      "Requirement already satisfied: pydantic-settings>=2.0.0 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from fastapi[standard]>=0.115.0->vllm) (2.12.0)\n",
      "Requirement already satisfied: pydantic-extra-types>=2.0.0 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from fastapi[standard]>=0.115.0->vllm) (2.11.0)\n",
      "Requirement already satisfied: jsonschema>=4.21.1 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from mistral_common>=1.8.5->mistral_common[image]>=1.8.5->vllm) (4.23.0)\n",
      "Requirement already satisfied: jmespath in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from model-hosting-container-standards<1.0.0,>=0.1.9->vllm) (1.0.1)\n",
      "Requirement already satisfied: setuptools in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from model-hosting-container-standards<1.0.0,>=0.1.9->vllm) (75.1.0)\n",
      "Requirement already satisfied: supervisor>=4.2.0 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from model-hosting-container-standards<1.0.0,>=0.1.9->vllm) (4.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from pydantic>=2.12.0->vllm) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from pydantic>=2.12.0->vllm) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from pydantic>=2.12.0->vllm) (0.4.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from requests>=2.26.0->vllm) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from requests>=2.26.0->vllm) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from requests>=2.26.0->vllm) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from requests>=2.26.0->vllm) (2024.12.14)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from tokenizers>=0.21.1->vllm) (0.36.0)\n",
      "Requirement already satisfied: colorama in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from tqdm->vllm) (0.4.6)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from transformers<5,>=4.56.0->vllm) (0.5.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from aiohttp->vllm) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from aiohttp->vllm) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from aiohttp->vllm) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from aiohttp->vllm) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from aiohttp->vllm) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from aiohttp->vllm) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from aiohttp->vllm) (1.18.3)\n",
      "Requirement already satisfied: httpx-sse>=0.4 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from mcp->vllm) (0.4.3)\n",
      "Requirement already satisfied: pyjwt>=2.10.1 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from pyjwt[crypto]>=2.10.1->mcp->vllm) (2.10.1)\n",
      "Requirement already satisfied: pywin32>=310 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from mcp->vllm) (311)\n",
      "Requirement already satisfied: sse-starlette>=1.6.1 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from mcp->vllm) (3.1.2)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm) (2.8.0)\n",
      "Requirement already satisfied: typer>=0.15.1 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.21.1)\n",
      "Requirement already satisfied: rich-toolkit>=0.14.8 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.17.1)\n",
      "Requirement already satisfied: fastapi-cloud-cli>=0.1.1 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from httpx<1,>=0.25.0->anthropic==0.71.0->vllm) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic==0.71.0->vllm) (0.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.21.1->vllm) (2024.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from jinja2>=3.1.5->fastapi[standard]>=0.115.0->vllm) (3.0.2)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from jsonschema>=4.21.1->mistral_common>=1.8.5->mistral_common[image]>=1.8.5->vllm) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from jsonschema>=4.21.1->mistral_common>=1.8.5->mistral_common[image]>=1.8.5->vllm) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from jsonschema>=4.21.1->mistral_common>=1.8.5->mistral_common[image]>=1.8.5->vllm) (0.22.3)\n",
      "Requirement already satisfied: pycountry>=23 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.5->mistral_common[image]>=1.8.5->vllm) (24.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from pydantic-settings>=2.0.0->fastapi[standard]>=0.115.0->vllm) (1.2.1)\n",
      "Requirement already satisfied: cryptography>=3.4.0 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from pyjwt[crypto]>=2.10.1->mcp->vllm) (46.0.3)\n",
      "Requirement already satisfied: networkx in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from torch>=1.7.0->compressed-tensors==0.12.2->vllm) (3.2.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from torch>=1.7.0->compressed-tensors==0.12.2->vllm) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from sympy==1.13.1->torch>=1.7.0->compressed-tensors==0.12.2->vllm) (1.3.0)\n",
      "Requirement already satisfied: click>=7.0 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from uvicorn>=0.12.0->uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (8.1.8)\n",
      "Requirement already satisfied: httptools>=0.6.3 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.7.1)\n",
      "Requirement already satisfied: websockets>=10.4 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (15.0.1)\n",
      "Requirement already satisfied: win32-setctime>=1.0.0 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from loguru->compressed-tensors==0.12.2->vllm) (1.2.0)\n",
      "Requirement already satisfied: cffi>=2.0.0 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from cryptography>=3.4.0->pyjwt[crypto]>=2.10.1->mcp->vllm) (2.0.0)\n",
      "Requirement already satisfied: rignore>=0.5.1 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.7.6)\n",
      "Requirement already satisfied: sentry-sdk>=2.20.0 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.49.0)\n",
      "Requirement already satisfied: fastar>=0.8.0 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.8.0)\n",
      "Requirement already satisfied: rich>=13.7.1 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (14.2.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from typer>=0.15.1->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.5.4)\n",
      "Requirement already satisfied: pycparser in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from cffi>=2.0.0->cryptography>=3.4.0->pyjwt[crypto]>=2.10.1->mcp->vllm) (2.22)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\machine learning\\large language models\\llm\\llmenv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "# 1. Install vLLM\n",
    "!pip install vllm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c7cbff2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vllm._C'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLM\n\u001b[0;32m      3\u001b[0m llm \u001b[38;5;241m=\u001b[39m LLM(model)\n",
      "File \u001b[1;32md:\\Machine Learning\\Large language Models\\LLM\\LLMenv\\Lib\\site-packages\\vllm\\__init__.py:74\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m MODULE_ATTRS:\n\u001b[0;32m     73\u001b[0m     module_name, attr_name \u001b[38;5;241m=\u001b[39m MODULE_ATTRS[name]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 74\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m__package__\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, attr_name)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Machine Learning\\Large language Models\\LLM\\LLMenv\\Lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Machine Learning\\Large language Models\\LLM\\LLMenv\\Lib\\site-packages\\vllm\\entrypoints\\llm.py:20\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TypeVar\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbeam_search\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     15\u001b[0m     BeamSearchInstance,\n\u001b[0;32m     16\u001b[0m     BeamSearchOutput,\n\u001b[0;32m     17\u001b[0m     BeamSearchSequence,\n\u001b[0;32m     18\u001b[0m     create_sort_beams_key_function,\n\u001b[0;32m     19\u001b[0m )\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     21\u001b[0m     CompilationConfig,\n\u001b[0;32m     22\u001b[0m     PoolerConfig,\n\u001b[0;32m     23\u001b[0m     ProfilerConfig,\n\u001b[0;32m     24\u001b[0m     StructuredOutputsConfig,\n\u001b[0;32m     25\u001b[0m     is_init_field,\n\u001b[0;32m     26\u001b[0m )\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompilation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CompilationMode\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     29\u001b[0m     ConvertOption,\n\u001b[0;32m     30\u001b[0m     HfOverrides,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     33\u001b[0m     TokenizerMode,\n\u001b[0;32m     34\u001b[0m )\n",
      "File \u001b[1;32md:\\Machine Learning\\Large language Models\\LLM\\LLMenv\\Lib\\site-packages\\vllm\\config\\__init__.py:6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattention\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AttentionConfig\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcache\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CacheConfig\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompilation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      7\u001b[0m     CompilationConfig,\n\u001b[0;32m      8\u001b[0m     CompilationMode,\n\u001b[0;32m      9\u001b[0m     CUDAGraphMode,\n\u001b[0;32m     10\u001b[0m     PassConfig,\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdevice\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeviceConfig\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mec_transfer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ECTransferConfig\n",
      "File \u001b[1;32md:\\Machine Learning\\Large language Models\\LLM\\LLMenv\\Lib\\site-packages\\vllm\\config\\compilation.py:23\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m     Range,\n\u001b[0;32m     18\u001b[0m     config,\n\u001b[0;32m     19\u001b[0m     get_hash_factors,\n\u001b[0;32m     20\u001b[0m     hash_factors,\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init_logger\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m current_platform\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m resolve_obj_by_qualname\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmath_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m round_up\n",
      "File \u001b[1;32md:\\Machine Learning\\Large language Models\\LLM\\LLMenv\\Lib\\site-packages\\vllm\\platforms\\__init__.py:257\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _current_platform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    256\u001b[0m     platform_cls_qualname \u001b[38;5;241m=\u001b[39m resolve_current_platform_cls_qualname()\n\u001b[1;32m--> 257\u001b[0m     _current_platform \u001b[38;5;241m=\u001b[39m \u001b[43mresolve_obj_by_qualname\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplatform_cls_qualname\u001b[49m\u001b[43m)\u001b[49m()\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;28;01mglobal\u001b[39;00m _init_trace\n\u001b[0;32m    259\u001b[0m     _init_trace \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(traceback\u001b[38;5;241m.\u001b[39mformat_stack())\n",
      "File \u001b[1;32md:\\Machine Learning\\Large language Models\\LLM\\LLMenv\\Lib\\site-packages\\vllm\\utils\\import_utils.py:122\u001b[0m, in \u001b[0;36mresolve_obj_by_qualname\u001b[1;34m(qualname)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;124;03mResolve an object by its fully-qualified class name.\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    121\u001b[0m module_name, obj_name \u001b[38;5;241m=\u001b[39m qualname\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 122\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, obj_name)\n",
      "File \u001b[1;32md:\\Machine Learning\\Large language Models\\LLM\\LLMenv\\Lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Machine Learning\\Large language Models\\LLM\\LLMenv\\Lib\\site-packages\\vllm\\platforms\\cuda.py:16\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParamSpec\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# import custom ops, trigger op registration\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattention\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AttentionBackendEnum\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init_logger\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'vllm._C'"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "\n",
    "llm = LLM(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7c0f744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'ab'[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7ef9dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
